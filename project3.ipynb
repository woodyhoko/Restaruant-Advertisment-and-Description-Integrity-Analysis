{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "id": "nM-reME-auBE",
        "colab_type": "code",
        "outputId": "79d989dc-a0ed-4c92-abbf-86a7e53772ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "from math import sqrt\n",
        "import random\n",
        "from keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape\n",
        "from keras.models import Model\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow\n",
        "\n",
        "STUDENT_ID = '23846183'\n",
        "\n\n",
        "# Function to calculate RMSE\n",
        "def rmse(pred, actual):\n",
        "    # Ignore nonzero terms.\n",
        "    pred = pred[actual.nonzero()].flatten()\n",
        "    actual = actual[actual.nonzero()].flatten()\n",
        "    return sqrt(mean_squared_error(pred, actual))\n",
        "\n\n",
        "def build_cfmodel(n_users, n_items, embed_size, output_layer='dot'):\n",
        "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
        "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
        "    \n",
        "    user_emb = Embedding(output_dim=embed_size, input_dim=n_users, input_length=1)(user_input)\n",
        "    user_emb = Reshape((embed_size,))(user_emb)\n",
        "    item_emb = Embedding(output_dim=embed_size, input_dim=n_items, input_length=1)(item_input)\n",
        "    item_emb = Reshape((embed_size,))(item_emb)\n",
        "    \n",
        "    if output_layer == 'dot':\n",
        "        model_output = Dot(axes=1)([user_emb, item_emb])\n",
        "    elif output_layer == 'mlp':\n",
        "        mlp_input = Concatenate()([user_emb, item_emb])\n",
        "\n",
        "        dense_1 = Dense(64, activation='relu')(mlp_input)\n",
        "        dense_1_dp = Dropout(0.15)(dense_1)\n",
        "        dense_2 = Dense(32, activation='relu')(dense_1_dp)\n",
        "        dense_2_dp = Dropout(0.15)(dense_2)\n",
        "        model_output = Dense(1)(dense_2_dp)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    model = Model(inputs=[user_input, item_input],\n",
        "                  outputs=model_output)\n",
        "    return model\n",
        " \n",
        "def build_deepwide_model(len_continuous, deep_vocab_lens, deep2_vocab_lens, len_wide, embed_size):\n",
        "    embed_size2 = 32\n",
        "    \n",
        "    input_list = []\n",
        "    continuous_input = Input(shape=(len_continuous,), dtype='float32', name='continuous_input')\n",
        "    input_list.append(continuous_input)\n",
        "\n",
        "    emb_list = []\n",
        "    for vocab_size in deep_vocab_lens:\n",
        "        embed_size = int(vocab_size**0.25)\n",
        "        _input = Input(shape=(1,), dtype='int32')\n",
        "        input_list.append(_input)\n",
        "        _emb = Embedding(output_dim=embed_size, input_dim=vocab_size, input_length=1)(_input)\n",
        "        _emb = Reshape((embed_size,))(_emb)\n",
        "        emb_list.append(_emb)\n",
        "    \n",
        "    for vocab_size in deep2_vocab_lens:\n",
        "        embed_size2 = int(vocab_size**0.25)\n",
        "        _input = Input(shape=(1,), dtype='int32')\n",
        "        input_list.append(_input)\n",
        "        _emb2 = Embedding(output_dim=embed_size2, input_dim=vocab_size, input_length=1)(_input)\n",
        "        _emb2 = Reshape((embed_size2,))(_emb2)\n",
        "        emb_list.append(_emb2)\n",
        "        \n",
        "    deep_input = Concatenate()(emb_list + [continuous_input])\n",
        "    dense_1 = Dense(256, activation='relu')(deep_input)\n",
        "    dense_1_dp = Dropout(0.3)(dense_1)\n",
        "    dense_2 = Dense(128, activation='relu')(dense_1_dp)\n",
        "    dense_2_dp = Dropout(0.3)(dense_2)\n",
        "    dense_3 = Dense(64, activation='relu')(dense_2_dp)\n",
        "    dense_3_dp = Dropout(0.3)(dense_3)\n",
        "\n",
        "    wide_input = Input(shape=(len_wide,), dtype='float32')\n",
        "    input_list.append(wide_input)\n",
        "    \n",
        "    fc_input = Concatenate()([dense_3_dp, wide_input])\n",
        "    dense_1 = Dense(8, activation='sigmoid')(fc_input)\n",
        "    model_output = Dense(1)(dense_1)\n",
        "    model = Model(inputs=input_list,\n",
        "                  outputs=model_output)\n",
        "    return model\n",
        "\n\n",
        "def get_continuous_features(df, continuous_columns):\n",
        "    continuous_features = df[continuous_columns].values\n",
        "    return continuous_features\n",
        "\n\n",
        "def get_top_k_p_combinations(df, comb_p, topk, output_freq=False):\n",
        "    def get_category_combinations(categories_str, comb_p=2):\n",
        "        categories = categories_str.split(', ')\n",
        "        return list(combinations(categories, comb_p))\n",
        "    all_categories_p_combos = df[\"item_categories\"].apply(\n",
        "        lambda x: get_category_combinations(x, comb_p)).values.tolist()\n",
        "    all_categories_p_combos = [tuple(t) for item in all_categories_p_combos for t in item]\n",
        "    tmp = dict(Counter(all_categories_p_combos))\n",
        "    sorted_categories_combinations = list(sorted(tmp.items(), key=lambda x: x[1], reverse=True))\n",
        "    if output_freq:\n",
        "        return sorted_categories_combinations[:topk]\n",
        "    else:\n",
        "        return [t[0] for t in sorted_categories_combinations[:topk]]\n",
        "\n\n",
        "def get_wide_features(df):\n",
        "    def categories_to_binary_output(categories):\n",
        "        binary_output = [0 for _ in range(len(selected_categories_to_idx))]\n",
        "        for category in categories.split(', '):\n",
        "            if category in selected_categories_to_idx:\n",
        "                binary_output[selected_categories_to_idx[category]] = 1\n",
        "            else:\n",
        "                binary_output[0] = 1\n",
        "        return binary_output\n",
        "    def categories_cross_transformation(categories):\n",
        "        current_category_set = set(categories.split(', '))\n",
        "        corss_transform_output = [0 for _ in range(len(top_combinations))]\n",
        "        for k, comb_k in enumerate(top_combinations):\n",
        "            if len(current_category_set & comb_k) == len(comb_k):\n",
        "                corss_transform_output[k] = 1\n",
        "            else:\n",
        "                corss_transform_output[k] = 0\n",
        "        return corss_transform_output\n",
        "\n",
        "    category_binary_features = np.array(df.item_categories.apply(\n",
        "        lambda x: categories_to_binary_output(x)).values.tolist())\n",
        "    category_corss_transform_features = np.array(df.item_categories.apply(\n",
        "        lambda x: categories_cross_transformation(x)).values.tolist())\n",
        "    return np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "HiEh4Fy2BK0H",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# item_df['item_attributes'][1]"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "PwMW6hhhg9FN",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    root_path = \"\"\n",
        "    tr_df = pd.read_csv(root_path + \"data/train.csv\")\n",
        "    val_df = pd.read_csv(root_path + \"data/valid.csv\")\n",
        "    te_df = pd.read_csv(root_path + \"data/test.csv\")\n",
        "\n",
        "    tr_ratings = tr_df.stars.values\n",
        "    val_ratings = val_df.stars.values\n",
        "\n",
        "    user_df = pd.read_json(root_path + \"data/user.json\")\n",
        "    item_df = pd.read_json(root_path + \"data/business.json\")\n",
        "    user_df = user_df.rename(index=str, columns={t: 'user_' + t for t in user_df.columns if t != 'user_id'})\n",
        "    item_df = item_df.rename(index=str, columns={t: 'item_' + t for t in item_df.columns if t != 'business_id'})\n",
        "    \n",
        "    tr_df[\"index\"] = tr_df.index\n",
        "    val_df[\"index\"]  = val_df.index\n",
        "    te_df[\"index\"] = te_df.index\n",
        "    tr_df = pd.merge(pd.merge(tr_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
        "    val_df = pd.merge(pd.merge(val_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
        "    te_df = pd.merge(pd.merge(te_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "MAz3bI-0aVJL",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Continuous features\n",
        "    print(\"Prepare continuous features...\")\n",
        "    continuous_columns = [\"user_average_stars\", \"user_cool\", \"user_fans\", \n",
        "                          \"user_review_count\", \"user_useful\", \"user_funny\",\n",
        "                          \"item_is_open\", \"item_latitude\", \"item_longitude\", \n",
        "                          \"item_review_count\", \"item_stars\",\n",
        "                          \"user_compliment_cool\", \"user_compliment_cute\", \"user_compliment_funny\",\n",
        "                          \"user_compliment_hot\"] #VALID RMSE:  1.0429724102351896\n",
        "#                           \"user_compliment_list\", \"user_compliment_more\",\n",
        "#                           \"user_compliment_note\", \"user_compliment_photos\", \"user_compliment_plain\",\n",
        "#                           \"user_compliment_profile\", \"user_compliment_writer\"]\n",
        "    tr_continuous_features = get_continuous_features(tr_df, continuous_columns)\n",
        "    val_continuous_features = get_continuous_features(val_df, continuous_columns)\n",
        "    te_continuous_features = get_continuous_features(te_df, continuous_columns)\n",
        "    scaler = StandardScaler().fit(tr_continuous_features)\n",
        "    tr_continuous_features = scaler.transform(tr_continuous_features)\n",
        "    val_continuous_features = scaler.transform(val_continuous_features)\n",
        "    te_continuous_features = scaler.transform(te_continuous_features)\n",
        "\n   "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare continuous features...\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "id": "_0SpX9Ov4gnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1bf068e-ce24-43cd-d51a-addb84c61470"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = re.compile(\"'(.*)'\")"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "erRGzFrnptEj",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item_df['item_attributes_Alcohol'] = item_df['item_attributes'].apply(lambda x: str(x['Alcohol']) if x != None and 'Alcohol' in x else '\\'none\\'')\n",
        "item_df['item_attributes_Alcohol'] = item_df['item_attributes_Alcohol'].apply(lambda x: pattern.findall(x)[0] if len(pattern.findall(x)) > 0 else x)\n",
        "item_df['item_attributes_Alcohol'] = item_df['item_attributes_Alcohol'].apply(lambda x: x.lower())\n",
        "item_df['item_attributes_WiFi'] = item_df['item_attributes'].apply(lambda x: str(x['WiFi']) if x != None and 'WiFi' in x else '\\'none\\'')\n",
        "item_df['item_attributes_WiFi'] = item_df['item_attributes_WiFi'].apply(lambda x: pattern.findall(x)[0] if len(pattern.findall(x)) > 0 else x)\n",
        "item_df['item_attributes_WiFi'] = item_df['item_attributes_WiFi'].apply(lambda x: x.lower())\n",
        "item_df['item_attributes_NoiseLevel'] = item_df['item_attributes'].apply(lambda x: str(x['NoiseLevel']) if x != None and 'NoiseLevel' in x else '\\'none\\'')\n",
        "item_df['item_attributes_NoiseLevel'] = item_df['item_attributes_NoiseLevel'].apply(lambda x: pattern.findall(x)[0] if len(pattern.findall(x)) > 0 else x)\n",
        "item_df['item_attributes_NoiseLevel'] = item_df['item_attributes_NoiseLevel'].apply(lambda x: x.lower())\n",
        "item_df['item_attributes_HasTV'] = item_df['item_attributes'].apply(lambda x: str(x['HasTV']) if x != None and 'HasTV' in x else '\\'none\\'')\n",
        "item_df['item_attributes_HasTV'] = item_df['item_attributes_HasTV'].apply(lambda x: pattern.findall(x)[0] if len(pattern.findall(x)) > 0 else x)\n",
        "item_df['item_attributes_HasTV'] = item_df['item_attributes_HasTV'].apply(lambda x: x.lower())"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "-LrHFZMEjWQr",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Deep features\n",
        "    # get the deep features to number representative\n",
        "    print(\"Prepare deep features...\")\n",
        "    item_deep_columns = [\"item_city\", \"item_postal_code\", \"item_state\"]\n",
        "    item_deep_vocab_lens = []\n",
        "    for col_name in item_deep_columns:\n",
        "        # transpose Category features to number\n",
        "        tmp = item_df[col_name].unique()\n",
        "        vocab = dict(zip(tmp, range(1, len(tmp) + 1)))\n",
        "        item_deep_vocab_lens.append(len(vocab) + 1)\n",
        "        item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x] if x in vocab else 0)\n",
        "    item_deep_idx_columns = [t + \"_idx\" for t in item_deep_columns]\n",
        "    item_to_deep_features = dict(zip(item_df.business_id.values, item_df[item_deep_idx_columns].values.tolist()))\n",
        "    tr_deep_features = np.array(tr_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
        "    val_deep_features = np.array(val_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n",
        "    te_deep_features = np.array(te_df.business_id.apply(lambda x: item_to_deep_features[x]).values.tolist())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare deep features...\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "id": "QTNpjIDV4jCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ca82c8f-6e87-4840-917d-d847cee19a1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Deep features\n",
        "    # get the deep features to number representative\n",
        "    print(\"Prepare deep2 features...\")\n",
        "    item_deep2_columns = [\"item_attributes_Alcohol\", \"item_attributes_WiFi\",\n",
        "                          \"item_attributes_NoiseLevel\", \"item_attributes_HasTV\"]\n",
        "    item_deep2_vocab_lens = []\n",
        "    for col_name in item_deep2_columns:\n",
        "        # transpose Category features to number\n",
        "        tmp = item_df[col_name].unique()\n",
        "        vocab = dict(zip(tmp, range(1, len(tmp) + 1)))\n",
        "        item_deep2_vocab_lens.append(len(vocab) + 1)\n",
        "        item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x] if x in vocab else 0)\n",
        "    item_deep2_idx_columns = [t + \"_idx\" for t in item_deep2_columns]\n",
        "    item_to_deep2_features = dict(zip(item_df.business_id.values, item_df[item_deep2_idx_columns].values.tolist()))\n",
        "    tr_deep2_features = np.array(tr_df.business_id.apply(lambda x: item_to_deep2_features[x]).values.tolist())\n",
        "    val_deep2_features = np.array(val_df.business_id.apply(lambda x: item_to_deep2_features[x]).values.tolist())\n",
        "    te_deep2_features = np.array(te_df.business_id.apply(lambda x: item_to_deep2_features[x]).values.tolist())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare deep2 features...\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "id": "ycjVMdZ9Mdko",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "424d95c8-bd49-4744-c041-8eccc10c8646"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Wide (Category) features\n",
        "    print(\"Prepare wide features...\")\n",
        "    #   Prepare binary encoding for each selected categories\n",
        "    all_categories = [category for category_list in item_df.item_categories.values for category in category_list.split(\", \")]\n",
        "    category_sorted = sorted(Counter(all_categories).items(), key=lambda x: x[1], reverse=True)\n",
        "    selected_categories = [t[0] for t in category_sorted[:500]]\n",
        "    selected_categories_to_idx = dict(zip(selected_categories, range(1, len(selected_categories) + 1)))\n",
        "    selected_categories_to_idx['unk'] = 0\n",
        "    idx_to_selected_categories = {val: key for key, val in selected_categories_to_idx.items()}\n",
        "    #   Prepare Cross transformation for each categories\n",
        "    top_combinations = []\n",
        "    top_combinations += get_top_k_p_combinations(tr_df, 2, 50, output_freq=False)\n",
        "    top_combinations += get_top_k_p_combinations(tr_df, 3, 30, output_freq=False)\n",
        "    top_combinations += get_top_k_p_combinations(tr_df, 4, 20, output_freq=False)\n",
        "    top_combinations = [set(t) for t in top_combinations]\n",
        "\n",
        "    tr_wide_features = get_wide_features(tr_df)\n",
        "    val_wide_features = get_wide_features(val_df)\n",
        "    te_wide_features = get_wide_features(te_df)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepare wide features...\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "id": "Uh3tEyD14kVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0299f5b2-b266-467e-ef5f-d521ff2d551a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    # Build input\n",
        "    tr_features = []\n",
        "    tr_features.append(tr_continuous_features.tolist())\n",
        "    tr_features += [tr_deep_features[:,i].tolist() for i in range(len(tr_deep_features[0]))]\n",
        "    tr_features += [tr_deep2_features[:,i].tolist() for i in range(len(tr_deep2_features[0]))]\n",
        "    tr_features.append(tr_wide_features.tolist()) # shape(5, 100000)\n",
        "    val_features = []\n",
        "    val_features.append(val_continuous_features.tolist())\n",
        "    val_features += [val_deep_features[:,i].tolist() for i in range(len(val_deep_features[0]))]\n",
        "    val_features += [val_deep2_features[:,i].tolist() for i in range(len(val_deep2_features[0]))]\n",
        "    val_features.append(val_wide_features.tolist())\n",
        "    te_features = []\n",
        "    te_features.append(te_continuous_features.tolist())\n",
        "    te_features += [te_deep_features[:,i].tolist() for i in range(len(te_deep_features[0]))]\n",
        "    te_features += [te_deep2_features[:,i].tolist() for i in range(len(te_deep2_features[0]))]\n",
        "    te_features.append(te_wide_features.tolist())\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "O0jEhvVH4nPY",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Model training\n",
        "    deepwide_model = build_deepwide_model(\n",
        "        len(tr_continuous_features[0]),\n",
        "        item_deep_vocab_lens, \n",
        "        item_deep2_vocab_lens,\n",
        "        len(tr_wide_features[0]),\n",
        "        embed_size=128)\n",
        "    deepwide_model.compile(optimizer='adagrad', loss='mse')"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "Dse-aH8Nznip",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    filepath=root_path + \"weights.best.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "#     checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True, mode='min')\n",
        "    callbacks_list = [checkpoint]\n",
        "\n",
        "    history = deepwide_model.fit(\n",
        "        tr_features, \n",
        "        tr_ratings, \n",
        "#         validation_split=0.2,\n",
        "        epochs=5, verbose=1, callbacks=callbacks_list)\n",
        "    \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "100000/100000 [==============================] - 11s 111us/step - loss: 1.3619\n",
            "Epoch 2/5\n",
            "  1408/100000 [..............................] - ETA: 11s - loss: 1.1287"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000/100000 [==============================] - 11s 108us/step - loss: 1.0926\n",
            "Epoch 3/5\n",
            "100000/100000 [==============================] - 11s 108us/step - loss: 1.0753\n",
            "Epoch 4/5\n",
            "100000/100000 [==============================] - 11s 114us/step - loss: 1.0647\n",
            "Epoch 5/5\n",
            "100000/100000 [==============================] - 10s 104us/step - loss: 1.0605\n"
          ]
        }
      ],
      "execution_count": 36,
      "metadata": {
        "id": "fcKZa92j43yn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "5a74ab48-a7a3-4e3b-8cab-0fbd4e9708ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#     deepwide_model.load_weights(root_path + \"weights.best.hdf5\")\n",
        "\n",
        "    # Make Prediction\n",
        "    y_pred = deepwide_model.predict(tr_features)\n",
        "    print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
        "    y_pred = deepwide_model.predict(val_features)\n",
        "    print(\"VALID RMSE: \", rmse(y_pred, val_ratings))\n",
        "    y_pred = deepwide_model.predict(te_features)\n",
        "    res_df = pd.DataFrame()\n",
        "    res_df['pred'] = y_pred[:, 0]\n",
        "    res_df.to_csv(\"{}.csv\".format(STUDENT_ID), index=False)\n",
        "    print(\"Writing test predictions to file done.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN RMSE:  1.0218507886600197\n",
            "VALID RMSE:  1.0319115912972927\n",
            "Writing test predictions to file done.\n"
          ]
        }
      ],
      "execution_count": 38,
      "metadata": {
        "id": "gSV52mibjR0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c8ce9ebc-4fe6-4652-9269-9fb5fde3ec83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#     deepwide_model.load_weights(root_path + \"weights.best.hdf5\")\n",
        "\n",
        "    # Make Prediction\n",
        "    y_pred = deepwide_model.predict(tr_features)\n",
        "    print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
        "    y_pred = deepwide_model.predict(val_features)\n",
        "    print(\"VALID RMSE: \", rmse(y_pred, val_ratings))\n",
        "    y_pred = deepwide_model.predict(te_features)\n",
        "    res_df = pd.DataFrame()\n",
        "    res_df['pred'] = y_pred[:, 0]\n",
        "    res_df.to_csv(\"{}.csv\".format(STUDENT_ID), index=False)\n",
        "    print(\"Writing test predictions to file done.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN RMSE:  1.019753528008388\n",
            "VALID RMSE:  1.0295121470638828\n",
            "Writing test predictions to file done.\n"
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {
        "id": "T_K5PM8fi91s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f870dfff-510d-4e6c-c18a-dd721dc9f9dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#     deepwide_model.load_weights(root_path + \"weights.best.hdf5\")\n",
        "\n",
        "    # Make Prediction\n",
        "    y_pred = deepwide_model.predict(tr_features)\n",
        "    print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
        "    y_pred = deepwide_model.predict(val_features)\n",
        "    print(\"VALID RMSE: \", rmse(y_pred, val_ratings))\n",
        "    y_pred = deepwide_model.predict(te_features)\n",
        "    res_df = pd.DataFrame()\n",
        "    res_df['pred'] = y_pred[:, 0]\n",
        "    res_df.to_csv(\"{}.csv\".format(STUDENT_ID), index=False)\n",
        "    print(\"Writing test predictions to file done.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN RMSE:  1.0203544432569915\n",
            "VALID RMSE:  1.0302868178347295\n",
            "Writing test predictions to file done.\n"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {
        "id": "Apgv3dMk5hM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8627745a-481e-4068-a85c-616275f3dac8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN RMSE:  1.0203544432569915\n",
        "VALID RMSE:  1.0302868178347295"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "oEs2WjYFB3Xj",
        "colab_type": "code",
        "colab": {}
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "project3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}